1）题目：
在本练习中，您将使用支持向量机来建立一个垃圾邮件分类器。
在本练习的前半部分，您将使用支持向量机(SVM)处理各种两维的样本数据集。
使用这些数据集进行实验将帮助您直观地了解支持向量机如何工作，以及如何使用带高斯核函数的SVM。
在后半部分的练习中，您将使用支持向量机来构建垃圾邮件分类器。
2）知识点概括：
有人也称SVM为大间距分类器（large margin classifier）
带正则化的优化目标：
假设函数：
核函数用来定义新的特征变量。将训练数据作为标记点，
利用核函数计算每个训练数据与其他训练数据之间的相似度得到新的特征向量。
这样，优化目标和假设函数变为：
核函数需要满足默塞尔定理（Mercer’s theorem）保证SVM优化运行正常，不会发散

偏差与方差问题：
当C较大时：可能产生低偏差、高方差问题，过拟合
当C较小时：可能产生高偏差、低方差问题，欠拟合

对于高斯核函数中的方差sigma：
较大时：特征变化较缓，可能会产生高偏差、低方差问题，欠拟合
较小时：特征变化较快，可能产生低偏差、高方差问题，过拟合

待定的：
参数C的选择
核函数的选择

模型选择：
当特征数量多、训练集数量较少时，一般选用逻辑回归或者不带核函数的SVM（线性核函数）
当特征数量少、训练集数量适中时，一般选用带高斯核函数的SVM
当特征数量少、训练集数量很大时，一般选用逻辑回归或者不带核函数的SVM（如果用高斯核函数可能过慢）

对于大部分情况神经网络表现都很好，但是训练慢。
且SVM是凸优化问题，因此总会找到一个全局最小值，不用担心局部极小的情况。

逻辑回归与SVM比较：逻辑回归对异常值敏感，SVM对异常值不敏感(抗噪能力强)——
支持向量机改变非支持向量样本并不会引起决策面的变化；但是逻辑回归中改变任何样本都会引起决策面的变化

3）大致步骤：
样本集1，首先可视化数据集，然后使用不同的C再用线性核函数的SVM训练模型并画出决策边界。

样本集2，可视化数据集，定义高斯函数，再使用高斯核SVM训练
（这里我还没搞清楚自定义核函数的要求，所以直接用内置的高斯核训练模型，
但是感觉不用了解，吴恩达说的一般也不会用很奇怪的核函数）

样本集3，可视化训练集和验证集，和样本集2一样采用高斯核函数，
但是这里尝试不同的C和sigma，选择在验证集上错误率最低的模型，最后画出决策边界

4）关于Python：
Scikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，
包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。
里面包含了SVM的程序，直接调用调节参数即可。

svm.SVC( ) 可以选择C值，以及核函数，调用之后先fit，再predict，注意predict时输入为一个二维数组，
因此在画等高线的时候需要先把网格展开成二维数组进行predict再重新组成网格画图。
在选择核函数时可以自己定义，例如：svm.SVC(kernel=my_kernel)，内置核函数默认为rbf高斯核，
形式为exp(-γ||x - x'||^ 2)，其中包含一个gamma关键词，gamma默认为1/n_features。

具体可参考sklearn中文文档：http://sklearn.apachecn.org/#/docs/5

在计算错误率时，可能会涉及两幅图像像素值之间的加减运算，这里需要注意的是图像像素值是ubyte类型，
ubyte类型数据范围为0~255，若做运算出现负值或超出255，则会抛出异常，
因此需要将预测的y值和yval强制转为int再进行相减。




